{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6: Intrinsic Dimension and Density Estimation\n",
    "You can use external libraries for linear algebra operations but you are expected to write your own algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Using the ```dry_beans_dataset``` as we did in previous laboratories (ie. follow the same proprocessing steps but **do not** perform the train-test split), program your own implementation of the two-NN estimate for the Intrinsic Dimension. \n",
    "\n",
    "Is the result comparible with what you would expect from an analysis of PCA's spectrum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../Datasets/Dry_Bean_Dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "y=np.array(y)\n",
    "encoder.fit(y.reshape(-1,1))\n",
    "y = encoder.transform(y.reshape(-1,1))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(pca.singular_values_)\n",
    "plt.title(\"Eigenvalues spectrum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadapy.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id, _, r = data.compute_id_2NN()\n",
    "#compute_id_2NN returns:\n",
    "## id: estimated instrinsic dimension\n",
    "## id_err: standard error on the id estimation\n",
    "## rs: the average nearest neighbor distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Intrisic Dimension = {id}\\nr = {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what happens if we remove the instances for which the distance to their first NN is null.\n",
    "\n",
    "**Note**: with ```sklearn.neighbors.NearestNeighbors``` the first NN is considered to be the point itself, so the first and second NNs are actually indexed by 1 and 2 (not 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.compute_distances(maxk=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(data.distances[:,1]==0)\n",
    "idx \n",
    "#as we said, it is only a small fraction of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id, _, r = data.compute_id_2NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Intrisic Dimension = {id}\\nr = {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Using the following code, create a one-dimensional dataset of size $N=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t #normal distribution, t distribution\n",
    "\n",
    "np.random.seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "X = np.concatenate(\n",
    "    (np.random.standard_t(1, int(0.04*N))-3.5,np.random.normal(5, 1, int(0.48 * N)), np.random.normal(7.5, 1, int(0.48 * N)))\n",
    ")[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = np.linspace(-12,12, 1000)[:, np.newaxis]\n",
    "true_dens = 0.04* t(df=1,loc=-3.5).pdf(X_plot[:, 0]) + 0.48* norm(5, 1).pdf(X_plot[:, 0]) + 0.48*norm(7.5,1).pdf(X_plot[:,0])\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the density estimation with your implementations of:\n",
    "- Histogram Density Estimation (Freedman-Diaconis rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3, figsize=(15,10))\n",
    "\n",
    "for i,bin in enumerate([2,5,12,24,50,100]):\n",
    "    ax = axes[i//3, i%3]\n",
    "    ax.hist(X, bins=bin)\n",
    "    ax.set_title(f\"Bins = {bin}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friedman_diaconis(X):\n",
    "    ''' \n",
    "    Select the number of bins such that bin=2*IQR(x)/(n^(1/3))\n",
    "    '''\n",
    "    q3, q1 = np.percentile(X, [75,25])\n",
    "    iqr = q3 - q1\n",
    "    return 2*iqr/(len(X)**(1/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_fd=friedman_diaconis(X)\n",
    "print(bin_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_number = int((max(X)-min(X))/bin_fd)\n",
    "print(bin_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.hist(X, bins=bin_number)\n",
    "plt.title(f\"Freedman Diaconis rule, bins = {bin_number}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel Density Estimation (KDE) - Gaussian kernel (Silverman's rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(20,12))\n",
    "\n",
    "ax[0].fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"True distribution\")\n",
    "ax[1].fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"True distribution\")\n",
    "\n",
    "\n",
    "colors=['tab:orange', 'tab:red', 'tab:blue','tab:cyan']\n",
    "\n",
    "for i,bandwidth in enumerate([0.1,0.5, 0.3, 0.8]):\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(X)\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    ax[i%2].plot(X_plot[:,0], np.exp(log_dens), lw=3, linestyle=\"-\", label=f\"b={bandwidth}\", c=colors[i])\n",
    "ax[0].legend(loc='upper left')\n",
    "ax[0].set_ylim(0, 0.3)\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_ylim(0, 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silverman(X):\n",
    "    '''\n",
    "    There is no built in function to compute the bandwidth with Silverman's rule in sklearn's framework \n",
    "    (see scipy.stat.gaussian_kde(dataset, bw_method='silverman') for an alternative)\n",
    "    \n",
    "    h = 0.9 * min(sigma, IQR/1.34)*n^(-1/5)\n",
    "    '''\n",
    "    \n",
    "    q3, q1 = np.percentile(X, [75,25])\n",
    "    iqr = q3 - q1\n",
    "    sigma = np.std(X)\n",
    "\n",
    "    return 0.9*min(sigma, iqr/1.34)*len(X)**(-1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silverman_bandwidth = silverman(X)\n",
    "print(silverman_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"True distribution\")\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=silverman_bandwidth).fit(X)\n",
    "log_dens = kde.score_samples(X_plot)\n",
    "plt.plot(X_plot[:,0], np.exp(log_dens), color=\"black\", lw=2, linestyle=\"-\", label=\"Gaussian kernel\")\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), \"+k\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
